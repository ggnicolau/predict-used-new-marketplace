{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3051cb-d36a-4458-918f-4b5f6512c208",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dede4ec6",
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "dfs = pd.read_json('~/Documents/4.MY_PROJECTS/GitHub/predict-used-new-marketplace/data/raw/MLA_100k_checked_v3.jsonlines', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6fd2c7-41ec-4176-abf3-fd637f740c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.rename(columns = {'tags':'tag'})\n",
    "dfs = dfs.rename(columns = {'id':'Id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa831a1-bdee-4760-b613-9cc453d89627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 used\n"
     ]
    }
   ],
   "source": [
    "used_count=dfs['title'].str.contains('usado').sum()\n",
    "if used_count>0:\n",
    "    print (\"There are {m} used\".format(m=mel_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ec605-1d02-41de-8242-d4bf25b7b80e",
   "metadata": {},
   "source": [
    "## Get features from dictionary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8250eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get region\n",
    "dfs['seller_country'] = dfs.apply(lambda x : x['seller_address']['country']['name'], axis = 1)\n",
    "dfs['seller_state'] = dfs.apply(lambda x : x['seller_address']['state']['name'], axis = 1)\n",
    "dfs['seller_city'] = dfs.apply(lambda x : x['seller_address']['city']['name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5195ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform id (named as descriptions) column to get data\n",
    "import ast\n",
    "def str_to_dict(column):\n",
    "    for i in range(len(column)):\n",
    "        try:\n",
    "            column[i] = ast.literal_eval(column[i][0])\n",
    "        except:\n",
    "            return\n",
    "\n",
    "str_to_dict(dfs['descriptions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e13079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data from descriptions and shipping \n",
    "dfs = pd.concat([dfs, dfs[\"descriptions\"].apply(pd.Series)], axis=1)\n",
    "dfs = pd.concat([dfs, dfs[\"shipping\"].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a1386-4858-40bd-b813-87aee3f88967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "dfs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3107e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get payment methods from dict\n",
    "def convertCol(x,key,i):\n",
    "    try:\n",
    "        return x[i][key]\n",
    "    except: \n",
    "        return ''\n",
    "    \n",
    "for key in ['description']: #['description','id','type'] -- only description is interesting\n",
    "    for i in range(0,13):\n",
    "        dfs[f'payment_{key}{i}'] = dfs['non_mercado_pago_payment_methods'].apply(lambda x: convertCol(x,key,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean column for each payment method \n",
    "lista_c = []\n",
    "for i in range(0,13):\n",
    "    lista = dfs[f'payment_description{i}'].unique()\n",
    "    lista_c.extend(lista)\n",
    "\n",
    "desc_uniques = set(lista_c)\n",
    "desc_uniques.remove('')\n",
    "desc_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6425291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename column for an improved dataframe (#TODO: Use apply for performance)\n",
    "for col in desc_uniques:\n",
    "    col_name=col.replace(' ','_')\n",
    "    dfs[col_name] = dfs.isin([col]).any(axis=1)\n",
    "\n",
    "# drop older columns\n",
    "dfs = dfs.drop(dfs.loc[:, 'payment_description0':'payment_description12'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35828981-d6f6-4cb7-bf35-382181018cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dfs = dfs.applymap(lambda x: x if x else np.nan)\n",
    "dfs = dfs.dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8858b01-9e78-4888-9d8b-2d4f024cd62d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3041b8-88d3-445e-8a9a-36da7b104e69",
   "metadata": {},
   "source": [
    "## Change type and filter columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac1804-2d17-4096-99c5-2cd4b121555c",
   "metadata": {
    "tags": []
   },
   "source": [
    "COLUMNS THAT MATTERS:\n",
    "- warranty (good and new products have different kind of warranties)\n",
    "- sub_status (when a product ad is suspended might be due it's condition)\n",
    "- base_price (price are different when used or new)\n",
    "- seller_id (different sellers might sell used or new items)\n",
    "- price (price again)\n",
    "- buying_mode (type of buying might implicate something)\n",
    "- parent_item_id (might have correlation between similar products)\n",
    "- last_updated (we'll check)\n",
    "- id (we'll check)\n",
    "- official_store_id (different stores sells different items and conditions)\n",
    "- accepts_mercadopago (new products might accept more payment methods)\n",
    "- original_price (price again)\n",
    "- currency_id (type of payment and currency might be due to the kind of seller and products)\n",
    "- title (keep title to find product)\n",
    "- automatic_relist (we'll check')\n",
    "- stop_time (time might influece)\n",
    "- status (status might influece)\n",
    "- video_id fica (we'll check, but might be videos for used products)\n",
    "- initial_quantity (a good feature, used products have low counts)\n",
    "- start_time (time again)\n",
    "- sold_quantity (quantity again)\n",
    "- available_quantity (quantity again)\n",
    "- seller_country, state, city (used or new ads might have imbalanced distribution between regions)\n",
    "- local_pick_up (being new or used might influence if local pickup is available)\n",
    "- free_shipping (big sellers for new products might be more capable of assuming free shipping)\n",
    "- Contra_reembolso fica (payment methods matters)\n",
    "- Giro_postal (stays)\n",
    "- mode fica (don't know what it is but it's full, 'not_specified' might be more common on used products)\n",
    "- tags (we'll check about tags)\n",
    "- tag (we'll check about tag)\n",
    "- date_created\n",
    "- category \n",
    "\n",
    "TRANSFORM COLUMNS (TYPE OF PAYMENTS):\n",
    "- Cheque_certificado\n",
    "- Mastercard_Maestro\n",
    "- Diners\n",
    "- Transferencia_bancaria\n",
    "- MercadoPago (será? duplicado com accepts mercadopago?)\n",
    "- Efectivo\n",
    "- Tarjeta_de_crédito (duplicado com outras colunas? mesclar colunas e preencher essa)\n",
    "- American_Express\n",
    "- MasterCard\n",
    "- Visa_Electron\n",
    "- Visa\n",
    "- Acordar_con_el_comprador\n",
    "\n",
    "COLUMNS WE DON'T NEED:\n",
    "- seller_address (too specific)\n",
    "- deals_ids (nothing relevant, we checked)\n",
    "- shipping (nothing relevant, we checked)\n",
    "- non_mercad_pago_etc (transformed)\n",
    "- site_id (too specific)\n",
    "- listin_type_id sai\n",
    "- description (nothing relevant, we checked, turned out to be id)\n",
    "- international_delivery_mode\n",
    "- pictures (nothing relevant, we checked)\n",
    "- thumbnail (nothing relevant, we checked)\n",
    "- secure_thumbnail (nothing relevant, we checked)\n",
    "- permalink (nothing relevant, we checked)\n",
    "- free_methods (nothing relevant, we checked)\n",
    "\n",
    "DOUBTS:\n",
    "- variations\n",
    "- attributes\n",
    "- dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f93f3-4118-4e8b-975a-126da59fcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "dfs = dfs.rename(columns = {'id':'descr_id', 'Id': 'id'})\n",
    "\n",
    "# Reorder columns\n",
    "dfs = dfs[['title', 'condition', 'warranty','initial_quantity', 'available_quantity', 'sold_quantity',\n",
    "                'sub_status', 'buying_mode', 'original_price', 'base_price', 'price', 'currency_id',\n",
    "                'seller_country', 'seller_state', 'seller_city', 'Giro_postal',  \n",
    "                'free_shipping', 'local_pick_up', 'mode', 'tags', 'tag',\n",
    "                'Contra_reembolso','Acordar_con_el_comprador', 'Cheque_certificado', 'Efectivo', 'Transferencia_bancaria', 'Tarjeta_de_crédito',\n",
    "                'Mastercard_Maestro', 'MasterCard', 'Visa_Electron', 'Visa', 'Diners', 'American_Express',\n",
    "                'status', 'automatic_relist',\n",
    "                'accepts_mercadopago', 'MercadoPago', \n",
    "                'id', 'descr_id', 'deal_ids', 'parent_item_id', 'category_id', 'seller_id', 'official_store_id', 'video_id',\n",
    "                'date_created', 'start_time', 'last_updated', 'stop_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ebc2d-0c5d-4efa-97cd-1f4940220f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['accepts_mercadopago'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc4d7b-2642-4342-90f5-ab0e5494e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['MercadoPago'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c4220-1bad-46a3-82df-63ed90f00ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge columns about same subjects\n",
    "dfs['accepts_mercadopago'] = dfs['accepts_mercadopago'].fillna(dfs['MercadoPago'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd24354-db22-44c2-8c71-9cd6c26c8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['MasterCard'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740b992-196f-4d97-96c9-01e304c1fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['MasterCard'] = dfs['Mastercard_Maestro'].fillna(dfs['MercadoPago'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f24d1-1084-40b9-95a3-5a4848aee4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Visa'] = dfs['Visa_Electron'].fillna(dfs['Visa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42361549-662b-4e02-9de0-cd7e24a382f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Tarjeta_de_crédito'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbd6fd-adee-4c98-bd15-761f8749adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Tarjeta_de_crédito'] = dfs['Tarjeta_de_crédito'].fillna(dfs['Visa'])\n",
    "dfs['Tarjeta_de_crédito'] = dfs['Tarjeta_de_crédito'].fillna(dfs['MasterCard'])\n",
    "dfs['Tarjeta_de_crédito'] = dfs['Tarjeta_de_crédito'].fillna(dfs['Diners'])\n",
    "dfs['Tarjeta_de_crédito'] = dfs['Tarjeta_de_crédito'].fillna(dfs['American_Express'])\n",
    "dfs['Tarjeta_de_crédito'] = dfs['Tarjeta_de_crédito'].fillna(dfs['Visa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630718ad-64e0-4aeb-9676-4ad94902d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Tarjeta_de_crédito'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b76a72-49f2-4d27-80bf-04c2f954dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.rename(columns = {'Tarjeta_de_crédito':'Aceptan_Tarjeta'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797236e6-f870-47f7-bb5a-486d9aa48dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop used columns\n",
    "dfs = dfs.drop(columns=['MercadoPago', 'Mastercard_Maestro', 'Visa_Electron'])\n",
    "dfs = dfs.drop(columns=['Visa', 'MasterCard', 'Diners', 'American_Express'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75136e-469f-42e3-be66-18e8b2c25051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat columns to access data\n",
    "def try_join(l):\n",
    "    try:\n",
    "        return ','.join(map(str, l))\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "\n",
    "dfs['sub_status'] = try_join(dfs['sub_status'])\n",
    "dfs['tags'] = try_join(dfs['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4faf34-6bde-4ea2-bb2f-6fa2a30706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ece06c-b084-401d-b8e0-f342f3c35761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacc6b4-243f-4621-9574-9fba258ab38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform some columns to boolean type\n",
    "dfs[['Giro_postal', 'free_shipping', 'local_pick_up', 'Contra_reembolso', \n",
    "     'Acordar_con_el_comprador', 'Cheque_certificado', 'Efectivo', \n",
    "     'Transferencia_bancaria', 'Aceptan_Tarjeta', 'automatic_relist']] = dfs[['Giro_postal', 'free_shipping', 'local_pick_up', 'Contra_reembolso', \n",
    "                                                          'Acordar_con_el_comprador', 'Cheque_certificado', 'Efectivo', \n",
    "                                                          'Transferencia_bancaria', 'Aceptan_Tarjeta', 'automatic_relist']].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a90333-6fcf-4e12-913e-1ab62102d298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform type of all columns\n",
    "dfs = dfs.astype({'title':'str',\n",
    "                  'condition': 'category', #bool\n",
    "                  'warranty': 'category',\n",
    "                  'initial_quantity': 'float', #int\n",
    "                  'available_quantity': 'float', #int\n",
    "                  'sold_quantity': 'float', #int\n",
    "                  'sub_status': 'category', #bool?\n",
    "                  'buying_mode': 'category',\n",
    "                  'original_price': 'float',\n",
    "                  'base_price': 'float',\n",
    "                  'price': 'float',\n",
    "                  'currency_id': 'category',\n",
    "                  'seller_country': 'category',\n",
    "                  'seller_state': 'category',\n",
    "                  'seller_city': 'category',\n",
    "                  'Giro_postal': 'bool',\n",
    "                  'free_shipping': 'bool',\n",
    "                  'local_pick_up': 'bool',\n",
    "                  'mode': 'category',\n",
    "                  'tags': 'category', #bool?\n",
    "                  #'tag': 'category',\n",
    "                  'Contra_reembolso': 'bool',\n",
    "                  'Acordar_con_el_comprador': 'bool',\n",
    "                  'Cheque_certificado': 'bool',\n",
    "                  'Efectivo': 'bool',\n",
    "                  'Transferencia_bancaria': 'bool',\n",
    "                  'Aceptan_Tarjeta': 'bool',\n",
    "                  'id': 'category',\n",
    "                  'descr_id': 'category',\n",
    "                  #'deal_ids': 'category',\n",
    "                  'parent_item_id': 'category',\n",
    "                  'category_id': 'category',\n",
    "                  'seller_id': 'category',\n",
    "                  'official_store_id': 'category',\n",
    "                  'video_id': 'category',\n",
    "                  #'date_created': 'datetime',\n",
    "                  # 'start_time': 'datetime',\n",
    "                  # 'last_updated': 'datetime',\n",
    "                  # 'stop_time': 'datetime',\n",
    "                  'status': 'category', #bool?\n",
    "                  'automatic_relist': 'bool'\n",
    "                                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d21aa3-7d08-45fd-9553-eb1caff4ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10307b-5423-4932-9648-bf55e4ab84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def missing_zero_values_table(df):\n",
    "        zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n",
    "        mz_table = mz_table.rename(\n",
    "        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n",
    "        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n",
    "        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)\n",
    "        mz_table['Data Type'] = df.dtypes\n",
    "        mz_table = mz_table[\n",
    "            mz_table.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "            \"There are \" + str(mz_table.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
    "        return mz_table\n",
    "\n",
    "missing_zero_values_table(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb187f1d-168b-478a-93ca-1d937090a67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(dfs['seller_country'].value_counts())\n",
    "dfs = dfs.drop(columns = 'seller_country') # We can drop Country column, it's always Argentina\n",
    "display(dfs['seller_city'].mode()[0])\n",
    "display(dfs['seller_state'].mode()[0])\n",
    "dfs['seller_city'] = dfs['seller_city'].fillna(dfs['seller_city'].mode()[0])\n",
    "dfs['seller_state'] = dfs['seller_state'].fillna(dfs['seller_state'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd0d17-0c91-4bcd-bf16-1bd3433bf488",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['accepts_mercadopago'] = dfs['accepts_mercadopago'].fillna(False)\n",
    "dfs['sold_quantity'] = dfs['sold_quantity'].fillna(0) # Is it ok to fill sold_quantity with 0? [VALIDATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628d890-be87-4a4d-b22f-805cabd14d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['warranty'] = dfs['warranty'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "dfs['warranty'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8efa55-636d-429e-8363-1cb9c54cf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_temp1 = dfs[dfs['warranty'].isnull()]\n",
    "df_temp1['warranty'] = False\n",
    "\n",
    "df_temp2 = dfs[~dfs['warranty'].isnull()]\n",
    "df_temp2['warranty'] = True\n",
    "\n",
    "frames = [df_temp1, df_temp2]\n",
    "dfs = pd.concat(frames)\n",
    "dfs = dfs.astype({'warranty':'bool'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67d9dd-00ec-4e9c-8505-f083808cf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['warranty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627bf76-ebf1-465e-b6f7-dd4bab012dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('number of sold_quantity', dfs.sold_quantity.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89860638-84c2-47a6-b69e-27761eccd4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_value_per_cat():\n",
    "    flag = dfs.select_dtypes(include=['category']).shape[1]\n",
    "    i = 0\n",
    "\n",
    "    while i <= flag:\n",
    "        print(dict(dfs.select_dtypes(include=['category']).iloc[:,i:i+1].nunique()))\n",
    "        i = i+1\n",
    "\n",
    "get_value_per_cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff252b2-0ba4-4ae3-8590-998bdb367d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1a3aa-af19-4cdb-8337-bbf7a420185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dfs['sub_status'] = dfs['sub_status'].str.replace('nan,','')\n",
    "dfs['sub_status'] = dfs['sub_status'].str.replace(',nan','')\n",
    "display(len(re.findall(r'suspended',dfs['sub_status'][1])))\n",
    "display(dfs['sub_status'].value_counts().value_counts())\n",
    "display(dfs.shape)\n",
    "\n",
    "# We concluded this column is useless: every row has the same count of the same value ('suspended')\n",
    "dfs = dfs.drop('sub_status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3401b-1b64-43c9-aedc-30f417c8b9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfs['tags'] = dfs['tags'].str.replace('nan,','')\n",
    "# dfs['tags'] = dfs['tags'].str.replace(',nan','')\n",
    "\n",
    "# from ast import literal_eval\n",
    "# dfs['tags'] = dfs['tags'].apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "# def deduplicate(column):\n",
    "#     flag = len(column)\n",
    "#     i = 0\n",
    "    \n",
    "#     while i <= flag:\n",
    "#         try:\n",
    "#             # 1. Convert into list of tuples\n",
    "#             tpls = [tuple(x) for x in column[i]]\n",
    "#             # 2. Create dictionary with empty values and\n",
    "#             # 3. convert back to a list (dups removed)\n",
    "#             dct = list(dict.fromkeys(tpls))\n",
    "#             # 4. Convert list of tuples to list of lists\n",
    "#             dup_free = [list(x) for x in lst]\n",
    "#             # Print everything\n",
    "#             column[i] = list(map(''.join, dup_free))\n",
    "#             # [[1, 1], [0, 1], [0, 1], [1, 1]]\n",
    "#             i = i+1\n",
    "#         except:\n",
    "#             return\n",
    "        \n",
    "# deduplicate(dfs['tags'])\n",
    "# display(dfs['tags'].value_counts().value_counts())\n",
    "# display(dfs.shape)\n",
    "# display(dfs['tag'].value_counts().value_counts())\n",
    "\n",
    "# Other useless colums -- all rows have the same values\n",
    "dfs = dfs.drop('tags', axis=1)\n",
    "dfs = dfs.drop('tag', axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea25320-aabd-4c7e-8865-2854228a4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('dataframe shape', dfs.shape)\n",
    "display('unique ids', dfs.id.nunique())\n",
    "display('number of sellers', dfs.seller_id.nunique())\n",
    "display('number of categories', dfs.category_id.nunique())\n",
    "\n",
    "#Drop useless column\n",
    "dfs = dfs.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0e072-31e2-40ea-94b5-49714d624b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_zero_values_table(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b2d07-0345-41ae-aed0-a994d54349db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.dropna(axis=1) # drop all columns with missing values (we checked and they are not necessary or have too many missing values to imput properly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bc117-93f1-4b5e-9529-14f5444aaa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with datetimes to create new features\n",
    "dfs['year_start'] = pd.to_datetime(dfs['start_time']).dt.year.astype('category')\n",
    "dfs['month_start'] = pd.to_datetime(dfs['start_time']).dt.month.astype('category')\n",
    "dfs['year_stop'] = pd.to_datetime(dfs['stop_time']).dt.year.astype('category')\n",
    "dfs['month_stop'] = pd.to_datetime(dfs['stop_time']).dt.month.astype('category')\n",
    "dfs['week_day'] = pd.to_datetime(dfs['stop_time']).dt.weekday.astype('category')\n",
    "#dfs['days_active'] = (dfs['start_time'] - dfs['stop_time']).dt.days\n",
    "dfs['days_active'] = [int(i.days) for i in (dfs.stop_time - dfs.start_time)]\n",
    "dfs['days_active'] = dfs['days_active'].astype('int')\n",
    "dfs = dfs.reset_index(drop=True)\n",
    "\n",
    "#dfs = dfs.drop(['date_created', 'start_time', 'last_updated', 'stop_time'], axis=1)\n",
    "boxplot = dfs.boxplot(column=['days_active'], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f6718-de17-49ec-9b9a-e8d5180e1512",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7ca09-8043-4567-a1eb-8ba68bc61c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "mylist = list(dfs.select_dtypes(include=['category']).columns)\n",
    "dfs[mylist] = dfs[mylist].apply(preprocessing.LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "corr = dfs.corr()\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "    .set_table_styles(magnify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50adfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "                \n",
    "dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f82eb-4060-4983-9eed-75ae0d70b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "condition = {'new': 0,'used': 1}\n",
    "\n",
    "#dfs.condition = [condition[item] for item in dfs.condition]\n",
    "\n",
    "\n",
    "dfs = dfs.sample(100000).reset_index(drop=True)\n",
    "#dfs = dfs.reset_index(drop=True)\n",
    "\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'category', 'bool']\n",
    "\n",
    "#X = dfs.select_dtypes(include=numerics).drop(columns=['condition'], axis=1)\n",
    "X = dfs.select_dtypes(include=numerics).drop(columns=['condition'], axis=1)\n",
    "#X = dfs.drop(columns='condition')\n",
    "y = dfs.condition\n",
    "\n",
    "# clf = LogisticRegression().fit(X, y)\n",
    "# result = permutation_importance(clf, X, y, n_repeats=10,\n",
    "#                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144cf971-014d-417b-a03f-b7b499ae30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(13).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099eae2-d2ec-4993-938f-af9d79f9e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = feat_importances.nlargest(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X, y)))\n",
    "\n",
    "result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=-1) # number of repeats 10\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "ax1.set_yticks(tree_indices)\n",
    "ax1.set_yticklabels(X.columns[tree_importance_sorted_idx])\n",
    "ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "ax2.boxplot(\n",
    "    result.importances[perm_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=X.columns[perm_sorted_idx],\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543f1f6-7c77-4c87-8db7-b31db041321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = reversed(X.columns[tree_importance_sorted_idx])\n",
    "list_2 = list(list_2)[0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa44eb9-7adb-4eed-8a79-5202bf004a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22293b29-e82a-401a-93ad-5b4f2a9cc0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature importance based on mean decrease in impurity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X, y)\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "display(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "#ax.set_xticks(tree_indices)\n",
    "#ax.set_xticklabels(X.columns)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1301063-e048-40aa-bb45-f637b4874acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_3 = forest_importances.nlargest(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac00e5-1fd7-4a38-a60d-d0f273814a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importance based on permutation\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    forest, X, y, n_repeats=10, random_state=42, n_jobs=-1 # n_repeats = 10 \n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "display(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=X.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "#ax.set_xticks(tree_indices)\n",
    "#ax.set_xticklabels(X.columns)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369aa219-5dc7-44f8-a3f8-d82c440388a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_4 = forest_importances.nlargest(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199eb69-50a6-45a6-99f7-f46bae0fad94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Handling multicollinearity\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# corr = spearmanr(X.dropna(how='any')).correlation\n",
    "\n",
    "# # Ensure the correlation matrix is symmetric\n",
    "# corr = (corr + corr.T) / 2\n",
    "# np.fill_diagonal(corr, 1)\n",
    "\n",
    "# # We convert the correlation matrix to a distance matrix before performing\n",
    "# # hierarchical clustering using Ward's linkage.\n",
    "# distance_matrix = 1 - np.abs(corr)\n",
    "# dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "# dendro = hierarchy.dendrogram(\n",
    "#     dist_linkage, labels=data.feature_names.tolist(), ax=ax1, leaf_rotation=90\n",
    "# )\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437dac4-2191-4e97-a108-f2127450f0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "# cluster_id_to_feature_ids = defaultdict(list)\n",
    "# for idx, cluster_id in enumerate(cluster_ids):\n",
    "#     cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "# selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "# X_train_sel = X_train[:, selected_features]\n",
    "# X_test_sel = X_test[:, selected_features]\n",
    "\n",
    "# clf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# clf_sel.fit(X_train_sel, y_train)\n",
    "# print(\n",
    "#     \"Accuracy on test data with features removed: {:.2f}\".format(\n",
    "#         clf_sel.score(X_test_sel, y_test)\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a501d1-8d51-4beb-8890-ec4b288c393b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# start_time = time.time()\n",
    "# img_shape = X.shape\n",
    "# importances = forest.feature_importances_\n",
    "# elapsed_time = time.time() - start_time\n",
    "\n",
    "# print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "# imp_reshaped = importances.reshape(img_shape)\n",
    "# plt.matshow(imp_reshaped, cmap=plt.cm.hot)\n",
    "# plt.title(\"Pixel importances using impurity values\")\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f33b19-7f08-4ed9-b9a0-c7126f239933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Splits the dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# # Defines the estimator used by the Boruta algorithm\n",
    "# estimator = RandomForestRegressor()\n",
    "\n",
    "# from boruta import BorutaPy\n",
    "# # Creates the BorutaPy object\n",
    "# boruta = BorutaPy(estimator = estimator, n_estimators = 'auto', max_iter = 100)\n",
    "# # Fits Boruta\n",
    "# boruta.fit(np.array(X_train), np.array(y_train))\n",
    "\n",
    "# Important features\n",
    "important = list(X.columns[boruta.support_])\n",
    "print(f\"Features confirmed as important: {important}\")\n",
    "# Tentative features\n",
    "tentative = list(X.columns[boruta.support_weak_])\n",
    "print(f\"Unconfirmed features (tentative): {tentative}\")\n",
    "# Unimportant features\n",
    "unimportant = list(X.columns[~(boruta.support_ | boruta.support_weak_)])\n",
    "print(f\"Features confirmed as unimportant: {unimportant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04023b71-fd4d-4973-b58b-104b219c98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_5 = important + tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e635692-3b1e-479d-9f81-dcb3ce11a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = set([y for x in [list_1.index, list_2, list_3.index, list_4.index, list_5] for y in x])\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e1a65-8f35-4958-ac7f-660f553a8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BorutaShap import BorutaShap\n",
    "# import tqdm as notebook_tqdm\n",
    "\n",
    "# # Creates a BorutaShap selector for regression\n",
    "# selector = BorutaShap(importance_measure = 'shap', classification = False)\n",
    "\n",
    "# # Fits the selector\n",
    "# selector.fit(X = X_train, y = y_train, n_trials = 100, sample = False, verbose = True)\n",
    "# # n_trials -> number of iterations for Boruta algorithm\n",
    "# # sample -> samples the data so it goes faster\n",
    "\n",
    "# # Display features to be removed\n",
    "# features_to_remove = selector.features_to_remove\n",
    "# print(features_to_remove)\n",
    "# # Removes them\n",
    "# X_train_boruta_shap = X_train.drop(columns = features_to_remove)\n",
    "# X_test_boruta_shap = X_test.drop(columns = features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1f707-e949-43cf-a959-9d1c925db836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file in write mode\n",
    "with open(r'selected_features.txt', 'w') as fp:\n",
    "    for item in selected_features:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
